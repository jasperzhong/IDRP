{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "Comparison_model = torch.load('model/singleComparison.pkl')\n",
    "Contingency_model = torch.load('model/singleContingency.pkl')\n",
    "Expansion_model = torch.load('model/singleExpansion.pkl')\n",
    "Temporal_model = torch.load('model/singleTemporal.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_PDTB(type, word_cnt={}):\n",
    "    with open(config.resourses.data_base_dir + \"im\"+ type +\"Corenlp\", \"r\") as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    stop_words = []\n",
    "    max_seq_len = config.model.seq_len\n",
    "\n",
    "    arg1_sents = []\n",
    "    arg2_sents = []\n",
    "    labels = []\n",
    "\n",
    "    for _ ,value in data.items():\n",
    "        label = value['Sense'].split('.')[0]\n",
    "        \n",
    "        if label == \"Comparison\":\n",
    "            label = 0\n",
    "        elif label == \"Contingency\":\n",
    "            label = 1\n",
    "        elif label == \"Expansion\":\n",
    "            label = 2\n",
    "        elif label == \"Temporal\":\n",
    "            label = 3\n",
    "        else:\n",
    "            continue\n",
    "        \n",
    "        \n",
    "        arg1_words = []\n",
    "        for word in value['Arg1']['Tokens']:\n",
    "            if not stop_words or word['Word'] not in stop_words:\n",
    "                arg1_words.append(word['Word'])\n",
    "                if not word_cnt.get(word['Word']):\n",
    "                    word_cnt[word['Word']] = 0\n",
    "                word_cnt[word['Word']] += 1\n",
    "\n",
    "        arg2_words = []\n",
    "        for word in value['Arg2']['Tokens']:\n",
    "            if not stop_words or word['Word'] not in stop_words:\n",
    "                arg2_words.append(word['Word'])\n",
    "                if not word_cnt.get(word['Word']):\n",
    "                    word_cnt[word['Word']] = 0\n",
    "                word_cnt[word['Word']] += 1\n",
    "\n",
    "\n",
    "        if len(arg1_words) < max_seq_len and len(arg2_words) < max_seq_len:\n",
    "            arg1_sents.append(arg1_words)\n",
    "            arg2_sents.append(arg2_words)\n",
    "            labels.append(label)\n",
    "        # loss samples: train(432)  test(34)  dev(36)\n",
    "\n",
    "    return arg1_sents, arg2_sents, labels, word_cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_up_word_dict():\n",
    "    word_to_id = {'OOV':0}\n",
    "    num_words = 1\n",
    "    word_cnt = {}\n",
    "    _, _, _, word_cnt = load_PDTB(\"Train\", word_cnt)\n",
    "    _, _, _, word_cnt = load_PDTB(\"Test\", word_cnt)\n",
    "    _, _, _, word_cnt = load_PDTB(\"Dev\", word_cnt)\n",
    "\n",
    "    # first 10k words\n",
    "    word_cnt = sorted(word_cnt.items(), key = lambda x:int(x[1]), reverse=True)\n",
    "    word_cnt = word_cnt[:config.model.top_words]\n",
    "\n",
    "    # build up word dict\n",
    "    for key, _ in word_cnt:\n",
    "        word_to_id[key] = num_words\n",
    "        num_words += 1\n",
    "    config.model.vocab_size = len(word_to_id) + 1\n",
    "    return word_to_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent_to_tensor(batch, word_to_id):\n",
    "    '''\n",
    "    Inputs:\n",
    "        batch: [B * T]   type:string list\n",
    "    Outpus:\n",
    "        tensor: [T * B]  type:tensor\n",
    "    '''\n",
    "    pad_len = config.model.seq_len\n",
    "    batch_size = len(batch)\n",
    "    \n",
    "    tensor = torch.zeros(pad_len, batch_size, dtype=torch.long)\n",
    "    for i in range(batch_size):\n",
    "        for j in range(len(batch[i])):\n",
    "            id = word_to_id.get(batch[i][j], 0)\n",
    "            tensor[j][i] = id\n",
    "\n",
    "    return tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from config import config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_id = build_up_word_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_arg1_sents, test_arg2_sents, test_labels, _ = load_PDTB(\"Test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = config.training.batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda is available\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\"\n",
    "choise = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(choise + \" is available\")\n",
    "device = torch.device(choise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zyc/jupyter/baseline/model.py:65: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters().\n",
      "  outputs1, hidden1 = self.lstm_1(embeded_1, None)\n",
      "/home/zyc/jupyter/baseline/model.py:66: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters().\n",
      "  outputs2, hidden2 = self.lstm_2(embeded_2, None)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 2, 2, 2, 2, 3, 2, 2, 0, 2, 1, 0, 2, 2, 2, 2, 1, 2, 0, 2, 2, 3, 1, 1, 2, 2, 1, 1, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 1, 2, 2, 0, 2, 2, 2, 2, 0, 2, 0, 2, 0, 0, 2, 2, 2, 1, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 1, 2, 0, 2, 0, 2, 0, 1, 0, 2, 2, 2, 0, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 1, 3, 0, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 0, 1, 2, 2, 0, 2, 2, 3, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 0, 2, 2, 2, 1, 2, 1, 1, 2, 0, 0, 2, 1, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 0, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 3, 2, 3, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 0, 2, 1, 2, 2, 2, 2, 0, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 1, 2, 1, 0, 2, 2, 2, 2, 1, 0, 2, 0, 2, 2, 2, 2, 2, 2, 2, 1, 2, 3, 2, 2, 2, 2, 0, 2, 2, 0, 0, 3, 1, 2, 2, 2, 2, 2, 2, 2, 3, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 2, 0, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 1, 1, 2, 0, 2, 2, 1, 2, 0, 2, 2, 2, 2, 2, 2, 2, 0, 0, 2, 2, 2, 0, 2, 0, 1, 2, 0, 0, 2, 0, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 1, 2, 1, 2, 2, 2, 0, 2, 2, 0, 2, 0, 2, 2, 2, 2, 1, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 1, 1, 2, 2, 2, 2, 2, 1, 0, 2, 0, 2, 2, 1, 2, 2, 0, 2, 0, 0, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 1, 2, 3, 2, 2, 0, 2, 2, 2, 2, 2, 2, 0, 0, 2, 1, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 0, 0, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 1, 0, 2, 2, 3, 3, 2, 2, 0, 0, 0, 2, 2, 1, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 1, 0, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 3, 2, 0, 2, 1, 2, 1, 0, 2, 2, 0, 2, 2, 1, 1, 2, 2, 1, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 1, 2, 1, 3, 2, 2, 2, 2, 0, 2, 0, 1, 2, 1, 1, 2, 0, 2, 2, 0, 0, 0, 1, 2, 0, 0, 2, 2, 2, 0, 2, 0, 2, 2, 2, 1, 2, 2, 0, 0, 2, 2, 0, 0, 2, 2, 2, 2, 2, 1, 0, 1, 2, 2, 2, 2, 2, 2, 3, 2, 2, 2, 2, 2, 2, 3, 2, 0, 2, 2, 0, 2, 2, 2, 2, 2, 2, 0, 0, 2, 2, 1, 2, 1, 2, 1, 2, 2, 0, 0, 2, 2, 2, 2, 0, 2, 2, 2, 0, 2, 2, 1, 0, 2, 2, 2, 2, 0, 1, 2, 2, 2, 2, 0, 2, 2, 0, 2, 2, 2, 1, 2, 2, 2, 2, 1, 2, 3, 2, 2, 2, 2, 2, 1, 2, 0, 2, 2, 2, 0, 1, 2, 2, 2, 2, 2, 2, 2, 0, 0, 2, 2, 3, 2, 2, 2, 0, 2, 2, 2, 2, 1, 2, 0, 1, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 1, 2, 2, 0, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 2, 2, 0, 2, 1, 2, 2, 2, 2, 2, 3, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 2, 2, 2, 0, 2, 2, 2, 2, 1, 0, 3, 2, 2, 2, 2, 0, 0, 3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 0, 0, 1, 2, 2, 2, 2, 2, 2, 1, 2, 3, 2, 2, 0, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 0, 3, 3, 2, 2, 0, 2, 2, 0, 2, 2, 2, 2, 2, 0, 2, 1, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 1, 1, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 2, 1, 1, 2, 2, 2, 2, 2, 1, 3, 2, 2, 2, 2, 2, 1, 2, 2, 1, 1, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 0, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 1, 0, 2, 2, 2, 2, 1, 2, 2, 2, 2, 3, 2, 1, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 3, 2, 2, 2, 3, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 0, 2, 1, 2, 2, 1, 1, 2, 2, 2, 2, 0, 2, 1, 2, 0, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 1, 1, 2, 2, 1, 0, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 0, 2, 2, 2, 1, 2, 1, 0, 3, 1, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 1, 2, 2, 2, 2, 0, 1, 2, 2, 2, 2, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 1, 2, 1, 2, 2, 2, 0, 2, 0, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 0, 1, 0, 0, 2, 2, 2, 0, 2, 2, 2, 0, 2, 2, 1, 2, 2, 2, 2, 2, 2, 3, 0, 0, 2, 2, 2, 2, 2, 2, 1, 1, 2, 1, 1, 2, 1, 2, 2, 0, 2, 2, 0, 3, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 1, 2, 2, 1, 1, 0, 2, 2, 1, 2, 2, 0, 2, 2, 3, 2, 2, 0, 2, 0, 0, 2, 2, 2, 2, 0, 2, 0, 2, 1, 2, 2, 1, 2, 2, 2, 2, 1, 1, 0, 2, 2, 2, 2, 2, 2, 2, 3, 2, 2, 0, 2, 2, 2, 1, 0, 0, 2, 0, 2, 2, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 1, 2, 2, 2, 0, 2, 2, 1, 2, 2, 2, 2, 1, 0, 2, 0, 2, 2, 2, 0, 2, 1, 2, 2, 2, 2, 1, 2, 2, 1, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 1, 2, 2, 2, 2, 2, 0, 1, 2, 2, 1, 1, 2, 2, 0, 1, 2, 2, 1, 2, 0, 2, 2, 0, 2, 0, 2, 2, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 1, 2, 2, 3, 2, 2, 1, 1, 2, 1, 2, 2, 2, 2, 2, 2, 1, 1, 2, 2, 2, 0, 2, 0, 2, 2, 2, 0, 2, 2, 2, 0, 2, 2, 0, 2, 0, 2, 1, 1, 0, 0, 2]\n",
      "[0, 0, 1, 2, 2, 2, 1, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 3, 3, 1, 1, 2, 0, 1, 2, 2, 1, 1, 0, 3, 2, 2, 2, 2, 2, 2, 2, 1, 3, 2, 2, 2, 2, 2, 2, 2, 2, 1, 0, 2, 2, 2, 2, 2, 2, 2, 1, 2, 3, 2, 1, 2, 0, 0, 2, 0, 2, 2, 1, 1, 1, 0, 2, 1, 2, 2, 2, 2, 3, 1, 2, 2, 2, 1, 2, 2, 2, 2, 1, 1, 1, 2, 1, 1, 1, 2, 1, 2, 0, 1, 1, 2, 1, 1, 0, 1, 1, 1, 2, 1, 2, 1, 2, 1, 2, 2, 2, 2, 0, 2, 0, 0, 2, 2, 2, 2, 2, 3, 2, 1, 1, 2, 1, 2, 2, 2, 1, 2, 2, 2, 1, 2, 2, 3, 1, 2, 2, 2, 1, 2, 2, 2, 1, 1, 2, 2, 2, 0, 1, 0, 2, 2, 1, 0, 2, 1, 1, 2, 2, 2, 2, 1, 2, 2, 2, 3, 2, 2, 2, 1, 2, 2, 2, 2, 2, 3, 1, 2, 2, 2, 0, 3, 0, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 1, 2, 1, 2, 2, 2, 1, 2, 0, 0, 2, 2, 2, 2, 0, 2, 2, 0, 2, 2, 2, 2, 1, 0, 3, 3, 1, 2, 2, 2, 2, 2, 2, 2, 1, 1, 0, 1, 2, 0, 2, 1, 2, 1, 2, 2, 2, 2, 2, 0, 2, 2, 0, 2, 2, 1, 2, 0, 2, 2, 2, 2, 0, 3, 0, 2, 2, 2, 2, 3, 1, 3, 1, 0, 2, 1, 2, 2, 2, 2, 0, 2, 1, 2, 2, 2, 3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 2, 0, 2, 2, 2, 0, 3, 0, 1, 2, 2, 1, 2, 2, 0, 2, 2, 2, 1, 3, 2, 2, 2, 1, 2, 2, 2, 0, 1, 2, 2, 1, 1, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 1, 1, 2, 2, 2, 2, 0, 2, 3, 1, 0, 3, 1, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 3, 1, 2, 0, 2, 2, 2, 2, 2, 2, 3, 1, 2, 2, 0, 2, 2, 2, 2, 2, 3, 2, 2, 2, 2, 2, 2, 2, 2, 1, 0, 2, 0, 2, 1, 2, 2, 0, 2, 0, 2, 3, 2, 3, 0, 0, 3, 2, 2, 2, 2, 2, 0, 2, 3, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 1, 2, 1, 2, 3, 2, 0, 2, 2, 2, 2, 1, 2, 2, 2, 3, 2, 2, 2, 2, 2, 1, 1, 1, 2, 1, 1, 2, 2, 2, 2, 2, 1, 2, 3, 3, 3, 2, 2, 2, 2, 2, 2, 2, 3, 2, 2, 2, 2, 2, 1, 2, 1, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 1, 2, 0, 2, 0, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 1, 0, 2, 2, 2, 2, 1, 3, 2, 2, 2, 2, 2, 2, 2, 0, 1, 1, 2, 0, 2, 1, 1, 2, 2, 1, 2, 2, 2, 2, 2, 2, 3, 2, 1, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 1, 2, 2, 2, 1, 2, 2, 0, 2, 2, 1, 0, 1, 0, 1, 2, 2, 2, 1, 0, 0, 1, 2, 2, 3, 2, 2, 2, 1, 2, 2, 2, 2, 2, 0, 0, 2, 2, 0, 1, 1, 3, 2, 2, 2, 2, 2, 3, 2, 2, 2, 2, 1, 3, 2, 2, 2, 2, 2, 2, 2, 3, 2, 3, 1, 2, 3, 1, 2, 2, 2, 2, 3, 2, 0, 3, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 1, 2, 0, 2, 1, 3, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 0, 2, 1, 1, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 1, 1, 2, 1, 2, 2, 1, 2, 2, 2, 2, 2, 1, 1, 2, 2, 2, 0, 0, 2, 2, 2, 2, 1, 2, 0, 2, 2, 2, 2, 0, 0, 0, 2, 2, 2, 1, 2, 2, 1, 2, 2, 1, 1, 2, 1, 2, 2, 1, 3, 2, 2, 2, 2, 2, 2, 2, 1, 1, 2, 1, 2, 2, 2, 2, 2, 2, 2, 1, 2, 0, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 1, 2, 1, 2, 0, 2, 2, 2, 3, 2, 0, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 2, 0, 2, 2, 2, 1, 0, 2, 2, 3, 2, 2, 2, 2, 0, 2, 2, 2, 2, 1, 0, 1, 2, 2, 1, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 0, 0, 3, 2, 2, 2, 2, 2, 1, 2, 2, 1, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 1, 0, 1, 2, 1, 1, 2, 1, 1, 3, 0, 1, 2, 2, 2, 1, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 0, 2, 0, 0, 2, 1, 2, 2, 2, 2, 2, 3, 2, 2, 3, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 1, 0, 2, 2, 2, 0, 2, 3, 2, 2, 2, 1, 2, 1, 2, 2, 2, 1, 2, 1, 1, 1, 1, 1, 2, 1, 2, 2, 2, 2, 1, 1, 2, 2, 2, 2, 3, 0, 2, 1, 1, 1, 2, 2, 2, 1, 2, 2, 0, 2, 2, 2, 2, 0, 1, 2, 2, 2, 0, 2, 2, 2, 2, 0, 2, 2, 2, 0, 1, 2, 2, 2, 1, 2, 2, 2, 2, 1, 2, 0, 2, 2, 1, 2, 1, 1, 2, 2, 1, 2, 0, 0, 2, 1, 2, 2, 2, 1, 2, 2, 2, 2, 1, 1, 3, 2, 3, 2, 0, 2, 2, 3, 2, 2, 2, 2, 0, 1, 2, 0, 1, 2, 2, 0, 2, 2, 2, 0, 2, 2, 0, 2, 2, 2, 2, 1, 2, 2, 1, 2, 1, 1, 1, 1, 2, 2, 2, 1, 3, 2, 2, 2, 0, 0, 0, 1, 0, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 1, 1, 2, 2, 2, 2, 2, 2, 1, 1, 1, 3, 3, 0, 0, 2, 2, 2, 2, 1, 2, 2, 0, 2, 3, 2, 2, 2, 2, 1, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 0, 0, 2, 1, 2, 2, 2, 1, 1, 2, 1, 1, 2, 1, 1, 2, 1, 1, 2, 2, 3, 3, 2, 1, 3, 2, 1, 2, 2, 1, 2, 2, 2, 1, 2, 2, 3, 2, 0, 2, 2, 1, 2, 3, 0, 3, 2, 2, 2, 2, 1, 1, 1, 3, 2, 2, 2, 2, 2, 2, 1, 2, 0, 2, 2, 2, 2, 2, 2, 0, 2, 2, 1, 2, 2, 1, 2, 2, 2, 2, 0, 0, 1, 1, 1, 2, 2, 2, 2, 2, 3, 2, 0, 2, 2, 1, 2, 3, 1, 2, 2, 2, 0, 2, 2, 2, 0, 2, 2, 2, 2, 1, 1, 1, 1, 2, 2, 1, 2, 2, 2, 1, 1, 2, 2, 2, 2, 2, 1, 2, 0, 2, 2, 2, 2, 2, 2, 1, 2, 1, 2, 2, 2, 1, 2, 2, 2, 1, 1, 1, 2, 2, 1, 2, 2, 1, 2, 0, 2, 1, 2, 1, 2, 0, 2, 2, 2, 2, 2, 2, 3, 2, 2, 2, 3, 2, 2, 2, 1, 1, 1, 2, 2, 1, 2, 2, 2, 1, 0, 2, 2, 1, 2, 2, 2, 0, 2, 2, 2, 1, 3, 2, 1, 2, 0, 2, 2, 1, 2, 2, 2, 1, 2, 2, 3, 2, 1, 3, 2, 2, 1, 2, 2, 2, 2, 1, 1, 2, 2, 1, 1, 1, 1, 2, 2, 2, 3, 2, 2, 1, 2, 1, 1, 2, 0, 2, 2, 0, 2, 2, 2, 3, 1, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 1, 2, 2, 2, 2, 2, 1, 2, 1, 2, 2, 1, 2, 0, 2, 1, 2, 3, 2, 1, 2, 2, 0, 0, 2, 1, 2, 1, 1, 0, 2, 2, 2, 0, 0, 1, 2, 2, 2, 2, 2, 1, 2, 0]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.10      0.15      0.12       149\n",
      "          1       0.25      0.14      0.18       290\n",
      "          2       0.67      0.75      0.71       954\n",
      "          3       0.06      0.02      0.03        82\n",
      "\n",
      "avg / total       0.50      0.53      0.51      1475\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    result = []\n",
    "    for i in range(0, len(test_arg1_sents), batch_size):\n",
    "        arg1 = test_arg1_sents[i:i+batch_size]\n",
    "        arg2 = test_arg2_sents[i:i+batch_size]\n",
    "        label = test_labels[i:i+batch_size]\n",
    "\n",
    "        arg1 = sent_to_tensor(arg1, word_to_id).to(device)\n",
    "        arg2 = sent_to_tensor(arg2, word_to_id).to(device)\n",
    "\n",
    "        output_Comparison = Comparison_model(arg1, arg2).transpose(0, 1)[1].cpu().numpy()\n",
    "        output_Contingency = Contingency_model(arg1, arg2).transpose(0, 1)[1].cpu().numpy()\n",
    "        output_Expansion = Expansion_model(arg1, arg2).transpose(0, 1)[1].cpu().numpy()\n",
    "        output_Temporal = Temporal_model(arg1, arg2).transpose(0, 1)[1].cpu().numpy()\n",
    "        \n",
    "        \n",
    "        for j in range(batch_size):\n",
    "            try:\n",
    "                x = np.argmax([output_Comparison[j], output_Contingency[j],output_Expansion[j], output_Temporal[j]])\n",
    "                result.append(x)\n",
    "            except IndexError:\n",
    "                continue\n",
    "    print(result)\n",
    "    print(test_labels)\n",
    "    print(classification_report(test_labels, result))\n",
    "     \n",
    "    \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "zyc",
   "language": "python",
   "name": "zyc"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
